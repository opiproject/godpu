# SPDX-License-Identifier: Apache-2.0
# Copyright (c) 2022-2023 Dell Inc, or its subsidiaries.
# Copyright (C) 2023 Intel Corporation
---
version: "3.7"
services:
  spdk:
    image: docker.io/opiproject/spdk:v24.01
    volumes:
      - /dev/hugepages:/dev/hugepages
      - /dev/shm:/dev/shm
      - /proc:/proc
      - /var/tmp:/var/tmp
    ports:
      - "9009:9009"
      - "4444:4444"
      - "5555:5555"
    privileged: true
    networks:
      - opi
    working_dir: /usr/libexec/spdk/scripts
    command: |
      sh -x -c 'sync; echo 1 > /proc/sys/vm/drop_caches  && \
            mkdir -p /mnt/huge && \
            grep hugetlbfs /proc/mounts || mount -t hugetlbfs nodev /mnt/huge && \
            echo 1024 > /proc/sys/vm/nr_hugepages && \
            grep "" /sys/kernel/mm/hugepages/hugepages-*/nr_hugepages && \
            dd if=/dev/zero of=/tmp/aio_bdev_file bs=512 count=64 && \
            echo -n NVMeTLSkey-1:01:MDAxMTIyMzM0NDU1NjY3Nzg4OTlhYWJiY2NkZGVlZmZwJEiQ: > /tmp/opikey.txt && \
            chmod 0600 /tmp/opikey.txt && \
            /usr/local/bin/spdk_tgt -m 0x1 -s 512 --no-pci -S /var/tmp |& tee /tmp/spdk.log & \
            for i in `seq 1 10`; do ./rpc.py spdk_get_version && break || sleep 1; done  && \
            ./rpc.py bdev_malloc_create -b Malloc0 64 512 && \
            ./rpc.py bdev_malloc_create -b Malloc1 64 512 && \
            ./rpc.py nvmf_create_transport -t TCP -u 8192 -m 4 -c 0  && \
            ./rpc.py nvmf_create_transport -t VFIOUSER && \
            ./rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1  && \
            ./rpc.py nvmf_subsystem_allow_any_host nqn.2016-06.io.spdk:cnode1 --disable && \
            ./rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a  `hostname -i` -f ipv4 -s 4444  && \
            ./rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a  `hostname -i` -f ipv4 -s 5555 --secure-channel && \
            ./rpc.py nvmf_subsystem_add_host nqn.2016-06.io.spdk:cnode1 nqn.2014-08.org.nvmexpress:uuid:feb98abe-d51f-40c8-b348-2753f3571d3c --psk /tmp/opikey.txt && \
            ./rpc_http_proxy.py 0.0.0.0 9009 spdkuser spdkpass'
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "python3 /usr/libexec/spdk/scripts/rpc.py spdk_get_version || exit 1"
        ]
      interval: 6s
      retries: 5
      start_period: 20s
      timeout: 10s

  jaeger:
    image: jaegertracing/all-in-one:1.53.0
    ports:
      - "16686:16686"
      - "4317:4317"
      - "4318:4318"
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - opi
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "nc -zv localhost 4317 && nc -zv localhost 4318 || exit 1"
        ]
      interval: 6s
      retries: 5
      start_period: 20s
      timeout: 10s

  frr:
    image: quay.io/frrouting/frr:9.1.0
    cap_add:
      - NET_ADMIN
      - SYS_ADMIN
      - SYS_MODULE
      - CAP_NET_RAW
    stdin_open: true
    tty: true
    volumes:
      - ./network/conf/frr.conf:/etc/frr/frr.conf
    networks:
      - opi
    command: |
      sh -x -c 'touch /etc/frr/vtysh.conf && \
            sed -i "s/bgpd=no/bgpd=yes/g" /etc/frr/daemons && \
            sed -i "s/127.0.0.1/0.0.0.0/g" /etc/frr/daemons && \
            ip link add name lo0 type dummy && \
            ifconfig lo0 10.0.0.1 netmask 255.255.255.255 up && \
            /etc/init.d/frr stop && \
            /usr/lib/frr/watchfrr -d -F traditional zebra bgpd staticd && \
            sleep infinity'

  redis:
    image: redis:7.2.3-alpine3.18
    networks:
      - opi
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]

  opi-spdk-server:
    image: docker.io/opiproject/opi-spdk-bridge:main
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
    volumes_from:
      - spdk
    networks:
      - opi
    depends_on:
      redis:
        condition: service_healthy
      jaeger:
        condition: service_healthy
      spdk:
        condition: service_healthy
    command: /opi-spdk-bridge -grpc_port=50051 -http_port=8082 -spdk_addr=/var/tmp/spdk.sock -redis_addr="redis:6379"
    healthcheck:
      test: grpcurl -plaintext localhost:50051 list || exit 1

  opi-smbios-server:
    image: docker.io/opiproject/opi-smbios-bridge:main
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
    networks:
      - opi
    depends_on:
      jaeger:
        condition: service_healthy
    command: /opi-smbios-bridge -grpc_port=50051 -http_port=8082
    healthcheck:
      test: grpcurl -plaintext localhost:50051 list || exit 1

  opi-evpn-server:
    image: docker.io/opiproject/opi-evpn-bridge:main
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
    cap_add:
      - NET_ADMIN
    networks:
      - opi
    command: /opi-evpn-bridge -grpc_port=50151 -http_port=8082 -redis_addr="redis:6379" -frr_addr="frr"
    healthcheck:
      test: grpcurl -plaintext localhost:50151 list || exit 1
    depends_on:
      redis:
        condition: service_healthy
      jaeger:
        condition: service_healthy
      frr:
        condition: service_started

  opi-evpn-test:
    build:
      context: .
    networks:
      - opi
    depends_on:
      opi-evpn-server:
        condition: service_healthy
    command: evpn create-vrf --name blue --vni 100 --vtep 10.0.0.100/24 --loopback 10.100.0.1/24 --addr opi-evpn-server:50151

  opi-smbios-test:
    build:
      context: .
    networks:
      - opi
    depends_on:
      opi-smbios-server:
        condition: service_healthy
    command: inventory get --addr opi-smbios-server:50051

  opi-storage-test:
    build:
      context: .
    networks:
      - opi
    depends_on:
      opi-spdk-server:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c", "-x"]
    command: |
      '/dpu storage test --addr opi-spdk-server:50051 && \
        nvmf0=$$(/dpu storage create backend nvme controller --addr=opi-spdk-server:50051 --id nvmf0 --multipath failover) && \
        /dpu storage get backend nvme controller --addr=opi-spdk-server:50051 --name "$$nvmf0" && \
        path0=$$(/dpu storage create backend nvme path tcp --addr=opi-spdk-server:50051 --controller "$$nvmf0" --id path0 --ip $$(getent hosts spdk | cut -d" " -f1) --port 4444 --nqn nqn.2016-06.io.spdk:cnode1 --hostnqn nqn.2014-08.org.nvmexpress:uuid:feb98abe-d51f-40c8-b348-2753f3571d3c) && \
        /dpu storage get backend nvme path --addr=opi-spdk-server:50051 --name "$$path0" && \
        ss0=$$(/dpu storage create frontend nvme subsystem --addr=opi-spdk-server:50051 --id subsys0 --nqn "nqn.2022-09.io.spdk:opitest1") && \
        ctrl0=$$(/dpu storage create frontend nvme controller tcp --addr=opi-spdk-server:50051 --id ctrl0 --ip "127.0.0.1" --port 4420 --subsystem "$$ss0") && \
        ns0=$$(/dpu storage create frontend nvme namespace --addr=opi-spdk-server:50051 --id namespace0 --volume "Malloc0" --subsystem "$$ss0") && \
        /dpu storage delete frontend nvme namespace --addr=opi-spdk-server:50051 --name "$$ns0" && \
        /dpu storage delete frontend nvme controller --addr=opi-spdk-server:50051 --name "$$ctrl0" && \
        /dpu storage delete frontend nvme subsystem --addr=opi-spdk-server:50051 --name "$$ss0"' && \
        /dpu storage delete backend nvme path --addr=opi-spdk-server:50051 --name $$path0 && \
        /dpu storage delete backend nvme controller --addr=opi-spdk-server:50051 --name "$$nvmf0" && \

  opi-test:
    image: docker.io/library/alpine:3.19
    networks:
      - opi
    depends_on:
      opi-evpn-test:
        condition: service_completed_successfully
      opi-smbios-test:
        condition: service_completed_successfully
      opi-storage-test:
        condition: service_completed_successfully
    command: |
      sh -c 'exit 0'

networks:
  opi:
